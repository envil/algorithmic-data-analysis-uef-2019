{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TRAINING_DATA_PATH = './data/toy-data-training.csv'\n",
    "TESTING_DATA_PATH = './data/toy-data-testing.csv'\n",
    "DATA_TYPE = [('id', 'i8'), ('v1', 'f8'), ('v2', 'f8'), ('v3', 'f8'), ('v4', 'f8'), ('y', 'S10')]\n",
    "\n",
    "\n",
    "def weigh(node1, node2):\n",
    "    node1_value = np.array(list(node1)[1:-1])\n",
    "    node2_value = np.array(list(node2)[1:-1])\n",
    "    return 1 / (1 + np.linalg.norm(node1_value - node2_value))\n",
    "\n",
    "train_data = np.genfromtxt(TRAINING_DATA_PATH, names=True, dtype=DATA_TYPE, delimiter=',')\n",
    "test_data = np.genfromtxt(TESTING_DATA_PATH, names=True, dtype=DATA_TYPE, delimiter=',')\n",
    "all_data = np.concatenate((train_data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w(13, 0) 0.11350416126511091\nw(13, 1) 0.14285714285714285\nw(13, 2) 0.5\nw(13, 3) 0.10351694645735657\nw(13, 4) 0.0671481457783844\nw(13, 5) 0.1023021629920016\nw(13, 6) 0.15438708879488486\nw(13, 7) 0.4142135623730951\nw(13, 8) 0.11519216806670013\nw(13, 9) 0.0722270279186898\nw(13, 10) 0.10171118008217982\nw(13, 11) 0.09009410830061464\nw(13, 13) 0.10960059084055324\nw(13, 14) 0.1639607805437114\nw(13, 15) 0.07619421680124755\nw(13, 16) 0.10351694645735657\nw(13, 17) 0.11606619483971768\nw(13, 18) 0.06780705896206128\n[(15, 0.1639607805437114), (8, 0.4142135623730951), (3, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "weight13 = []\n",
    "for x in range(19):\n",
    "    if x == 12:\n",
    "        continue\n",
    "    weight13.append((x + 1, weigh(all_data[12], all_data[x])))\n",
    "    print('w({}, {})'.format(13, x), weigh(all_data[12], all_data[x]))\n",
    "print(sorted(weight13, key=lambda x: x[1])[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the 3 node 15, 8, and 3 are connected with node 13. 8 and 3 are known, so we discover node 15 to see which class it's in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w(15, 0) 0.23166247903554\nw(15, 1) 0.4142135623730951\nw(15, 2) 0.16666666666666666\nw(15, 3) 0.21712927295533244\nw(15, 4) 0.1\nw(15, 5) 0.1951941016011038\nw(15, 6) 0.21089672205953397\nw(15, 7) 0.14285714285714285\nw(15, 8) 0.14459058185587106\nw(15, 9) 0.10351694645735657\nw(15, 10) 0.1463924816619788\nw(15, 11) 0.15438708879488486\nw(15, 12) 0.1639607805437114\nw(15, 13) 0.21089672205953397\nw(15, 15) 0.10745035092526581\nw(15, 16) 0.179128784747792\nw(15, 17) 0.13100580420257674\nw(15, 18) 0.09584069468246141\n[(4, 0.21712927295533244), (1, 0.23166247903554), (2, 0.4142135623730951)]\n"
     ]
    }
   ],
   "source": [
    "weight15 = []\n",
    "for x in range(19):\n",
    "    if x == 14:\n",
    "        continue\n",
    "    weight15.append((x + 1, weigh(all_data[14], all_data[x])))\n",
    "    print('w({}, {})'.format(15, x), weigh(all_data[14], all_data[x]))\n",
    "print(sorted(weight15, key=lambda x: x[1])[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that node 15 are connected to node 4, 1, and 2. Since node 1, 2 are blue, we can conclude that node 15 are blue.\n",
    "\n",
    "Back to the node 13, since node 3, 15 are blue, => node 13 are blue\n",
    "# Task 2\n",
    "## Uncertainty sampling\n",
    "In uncertainty sampling, We will try to select the most informative data points, i.e. those will make the entropy highest, thus makes the uncertainty highest.\n",
    "In SVM, arcording to the geometric distribution of the support vectors and SVM theory, such points are the points close to the sperating hyperplane and two boundary line. [1]\n",
    "\n",
    "## Expected error reduction\n",
    "In expected error reduction stagegy, instead of maximizing the uncertainty of the queried instance like in uncertainty sampling, it tries to minimize the uncertainty on the remaining instances. In other words, they're two different approaches but have the same motivation and effect of the process. By this, the remaining points will be the points furthest points from the hyperplane of the SVM. Thus, the selected for query points are the points closest to the hyperplane of SVM, i.e. the support vectors.\n",
    "\n",
    "# Task 5\n",
    "- I will choose transductive SVM algorithm. I can use kernel trick to project the data to higher dimension space so that it can be separated easier. Slide ADA partI, page 42/63.\n",
    "- EM algo is not good for concentric rings data.\n",
    "- Graph-based approach for semisupervised classification might work but if the two rings are too close to each other then it might get messed up.\n",
    "\n",
    "# Task 6\n",
    "- Linear SVM are stable, which is not suitable for bagging [2]. Poor predictors can be transformed into worse one by bagging [3].\n",
    "\n",
    "- Kernal SVM performs better with bagging [2].\n",
    "\n",
    "- Boosting and the RSM may also be advantageous for linear classifiers [4], => must be good with linear SVM.\n",
    "- Boosting is also good with kernel SVM [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [An Uncertainty sampling-based Active Learning Approach\n",
    "For Support Vector Machines ](https://ieeexplore-ieee-org.ezproxy.uef.fi:2443/stamp/stamp.jsp?tp=&arnumber=5376609)\n",
    "\n",
    "[2] [Demonstrating the Stability of Support\n",
    "Vector Machines for Classification](http://ikee.lib.auth.gr/record/115086/files/Buciu_i06a.pdf)\n",
    "\n",
    "[3] [Bagging Boosting and C.45](http://home.eng.iastate.edu/~julied/classes/ee547/Handouts/q.aaai96.pdf)\n",
    "\n",
    "[4] [Bagging, Boosting and the Random Subspace Method for Linear Classifiers](http://rduin.nl/papers/paa_02_bagging.pdf)\n",
    "\n",
    "[5] [From Kernel Machines to Ensemble Learning](https://arxiv.org/pdf/1401.0767.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
